<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>OpenMLS Performance | OpenMLS</title>
<meta name=keywords content="mls,openmls,performance"><meta name=description content="In this blog post, I explore how MLS&rsquo; aspiration to be scalable manifests itself in OpenMLS.
The charter in particular claims the following:
Resource requirements have good scaling in the size of the group (preferably sub-linear)
While performance can be theoretically analysed for MLS it is also interesting to see whether the performance goals hold up in a real implementation. This of course only looks at a single implementation. Nonetheless, I think that it gives a good impression on the actual performance of MLS implementations."><meta name=author content="Franziskus Kiefer"><link rel=canonical href=https://blog.openmls.tech/posts/2021-05-18-openmls-first-benchmarks/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.openmls.tech/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.openmls.tech/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.openmls.tech/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.openmls.tech/apple-touch-icon.png><link rel=mask-icon href=https://blog.openmls.tech/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.openmls.tech/posts/2021-05-18-openmls-first-benchmarks/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="OpenMLS Performance"><meta property="og:description" content="In this blog post, I explore how MLS&rsquo; aspiration to be scalable manifests itself in OpenMLS.
The charter in particular claims the following:
Resource requirements have good scaling in the size of the group (preferably sub-linear)
While performance can be theoretically analysed for MLS it is also interesting to see whether the performance goals hold up in a real implementation. This of course only looks at a single implementation. Nonetheless, I think that it gives a good impression on the actual performance of MLS implementations."><meta property="og:type" content="article"><meta property="og:url" content="https://blog.openmls.tech/posts/2021-05-18-openmls-first-benchmarks/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-18T00:00:00+00:00"><meta property="article:modified_time" content="2021-05-18T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="OpenMLS Performance"><meta name=twitter:description content="In this blog post, I explore how MLS&rsquo; aspiration to be scalable manifests itself in OpenMLS.
The charter in particular claims the following:
Resource requirements have good scaling in the size of the group (preferably sub-linear)
While performance can be theoretically analysed for MLS it is also interesting to see whether the performance goals hold up in a real implementation. This of course only looks at a single implementation. Nonetheless, I think that it gives a good impression on the actual performance of MLS implementations."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.openmls.tech/posts/"},{"@type":"ListItem","position":2,"name":"OpenMLS Performance","item":"https://blog.openmls.tech/posts/2021-05-18-openmls-first-benchmarks/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"OpenMLS Performance","name":"OpenMLS Performance","description":"In this blog post, I explore how MLS\u0026rsquo; aspiration to be scalable manifests itself in OpenMLS.\nThe charter in particular claims the following:\nResource requirements have good scaling in the size of the group (preferably sub-linear)\nWhile performance can be theoretically analysed for MLS it is also interesting to see whether the performance goals hold up in a real implementation. This of course only looks at a single implementation. Nonetheless, I think that it gives a good impression on the actual performance of MLS implementations.","keywords":["mls","openmls","performance"],"articleBody":"In this blog post, I explore how MLSâ€™ aspiration to be scalable manifests itself in OpenMLS.\nThe charter in particular claims the following:\nResource requirements have good scaling in the size of the group (preferably sub-linear)\nWhile performance can be theoretically analysed for MLS it is also interesting to see whether the performance goals hold up in a real implementation. This of course only looks at a single implementation. Nonetheless, I think that it gives a good impression on the actual performance of MLS implementations. Particularly because OpenMLS at this point is not optimised but rather implements the MLS spec as is.\nMethodology MLS is a pretty complex protocol with many moving parts. It is therefore important to clearly define what is being measured and how.\nFirst, all tests are done with the only mandatory cipher suite in MLS 1.0 MLS10_128_DHKEMX25519_AES128GCM_SHA256_Ed25519. While other cipher suites obviously have different performance, the goal here is to investigate the general performance of MLS depending on the group size. The exact cipher suite used is therefore irrelevant.\nMeasurements The measurements here do not cover all possible messages in MLS. Not all of them are fully supported by OpenMLS yet. Pre-shared key, re-init, external-init, app-ack, and external proposals will be checked once they are implemented. The measured messages nonetheless represent the core of the MLS protocol and should give a good idea of the general performance of the protocol. We test performance of group creation, group join as well as the three basic messages update, add, and remove, and application messages.\nAll measurements except for the first two use one of the following set-ups:\nBase: The group is created by a user. All other participants are invited and each participant creates the group locally. Then every participant sends an update message to the group and everyone else processes it.\nBare: The group is created by a user. All other participants are invited and each participant creates the group locally. This creates an extremely sparse version of the underlying tree in MLS and is therefore interesting to look at.\nMeasurements are run on different group sizes. When running benchmarks with large groups such as 1000 participants a lot of memory is used in order to simulate all devices (up to 10 GB) such that larger groups are hard to simulate. The chosen group sizes allow us to get a good idea how MLS performs depending on the group size. We in particular test groups of the size 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000.\nOperations Group creation: Creating a group involves creating the group, proposals and welcome messages for the other participants, and applying the commit. Join group: Joining a group is equivalent to processing a welcome message to locally create the new group. Update messages: Sending an update to a group involves creating a proposal, the corresponding commit and applying the commit. When receiving an update message the commit is being processed. Adding a user: When a new user is added to the group the add proposal and welcome message are created and the commit is locally applied by the adder. When receiving a commit with an add proposal it is processed by the user. Removing a user: When a user is removed from the group the remove proposal and commit are created and locally applied by the remover. When receiving a commit with a remove proposal it is processed by the user. Application messages: Sending an application message consists of creating the plaintext message and encrypting it for the group. In order to receive an application message the user has to decrypt and parse the message. We measure performance of a single message that is being sent and processed. Note that the processing time of subsequent messages is not significantly different from the first one. Results You can find the raw data and some more graphs in the OpenMLS performance spreadsheet.\nAll measurements were performed on a laptop with Arch Linux, an Intel Core i7-4900MQ @ 2.80GHz and 16 GB memory.\nGroup setup As the following graph shows the time needed to create a group is linear in the number of participants added when creating the group. The blue line shows the actual measurements while the magenta one is a trend line showing the linear relation. This is what is to be expected because the performance is dominated by the creation of welcome messages, which have to be created for each member.\nJoin group The performance of joining a group is linear in the group size because the information in the welcome message as well as the tree that is being processed when joining the group are linear in the number of group members. Note that it is not logarithmic because the tree needs to be constructed. This requires processing of each node in some way, which is linear in the group size. The blue line again shows the actual measurements while the magenta one is a trend line for the linear relation.\nUpdate Sending and processing updates are both sub-linear in the number of group members because the number of computations depend on the height of the tree in the base case.\nIn the case of a very sparse tree, which we have in the bare case because every leaf only processed the welcome message, the performance of sending an update however is linear in the group size. When creating a commit for an update proposal the sender has to include a path and refresh the private tree. The following two flamegraphs show the difference between the base and the bare case. While it doesnâ€™t show directly whatâ€™s going on, it can be seen that in the base case (first flamegraph) the new_with_keys function requires a lot more time relative to the rest of the replace_private_tree function. This is a strong indicator for where to look for the differences.\nLooking at a tree with 300 leaves for example we have to encrypt 299 times (for every other leaf) in the case of a bare tree. In a fully updated tree however only 9 encryptions are necessary, one for each level of the tree. It is therefore expected that the performance of sending an update (with commit) in the bare case is linear in the group size.\nAdding a user Looking at the performance of adding a user and processing an add commit in the following graph we can again see the linear growth in relation to the number of group members. This is almost independent of the state of the tree. The operations appear to be slightly more expensive in a fully updated tree though.\nRemoving a user Like updating, removing a user and processing a remove commit are linear in complexity in the base case as the following graph shows. Removing in a very sparse tree is significantly more expensive than in a fully updated tree. The reason is the same as for updating the tree. The remove information has to be encrypted to all other remaining participants in the tree.\nApplication messages Sending and receiving application messages is essentially independent of the group size, as expected. Receiving the first message within an epoch has a small overhead compared to subsequent message as seen in the second graph. This should be negligible in practice though.\nAnalysis First, the plain performance numbers tell us that the goal of the MLS charter of a protocol that scales well for large groups has been mostly. Depending on the state of the tree some operations might take longer than expected. However, this can be mitigated by the application ensuring that the tree is updated and shrunk regularly. Notably, the real world performance appears to be consistent with the theoretical expectations.\nTechnical background The measurements are not done with any Rust benchmarking framework such as criterion. Due to the way criterion works thereâ€™s significant overhead in criterion. While the numbers in this post can be reliably reproduced a more thorough measurement framework\nThe flamegraphs are produced with pprof, a simple to use CPU profiler for Rust.\nAll measurements were performed on this revision. To reproduce them check out the revision and run\nfor i in 2 3 4 5 6 7 8 9 10 20 30 40 50 100 200 300 400 500 1000; do \\ cargo bench --bench group -- $i; \\ done Conclusion \u0026 Future work Measuring performance of a protocol as complex as MLS is pretty difficult. Without an application and elaborate test framework that can simulate many different scenarios it is only possible to get the basic numbers as shown here. While they give a good indicator towards the performance of the MLS protocol they are insufficient to claim any performance of real applications that use MLS.\nNonetheless, the numbers show that the MLS protocol appears to allow for efficient, end-to-end-encrypted messaging in large groups. Sending and receiving application messages is independent of the group size while group operations are sub-linear in the group size in most cases.\nWhen OpenMLS is further developed and we have a messaging client using it another set of measurements should be performed with real world usage scenarios in mind in order to investigate whether the performance we have seen here translates to efficient group messaging in an application. The MLS specification further leaves anything around authentication and authorization policies open to the application. These might be complex procedures and impact the MLS performance as well.\n","wordCount":"1580","inLanguage":"en","datePublished":"2021-05-18T00:00:00Z","dateModified":"2021-05-18T00:00:00Z","author":{"@type":"Person","name":"Franziskus Kiefer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.openmls.tech/posts/2021-05-18-openmls-first-benchmarks/"},"publisher":{"@type":"Organization","name":"OpenMLS","logo":{"@type":"ImageObject","url":"https://blog.openmls.tech/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.openmls.tech/ accesskey=h title="OpenMLS (Alt + H)">OpenMLS</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.openmls.tech/ title=Home><span>Home</span></a></li><li><a href=https://github.com/openmls/openmls title=Github><span>Github</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">OpenMLS Performance</h1><div class=post-meta><span title='2021-05-18 00:00:00 +0000 UTC'>May 18, 2021</span>&nbsp;Â·&nbsp;Franziskus Kiefer</div></header><div class=post-content><p>In this blog post, I explore how MLS&rsquo; aspiration <a href=https://datatracker.ietf.org/doc/charter-ietf-mls/>to be scalable</a> manifests itself in OpenMLS.</p><p>The charter in particular claims the following:</p><blockquote><p>Resource requirements have good scaling in the size of the group (preferably sub-linear)</p></blockquote><p>While performance can be theoretically analysed for MLS it is also interesting to see whether the performance goals hold up in a real implementation.
This of course only looks at a single implementation.
Nonetheless, I think that it gives a good impression on the actual performance of MLS implementations.
Particularly because OpenMLS at this point is not optimised but rather implements the MLS spec as is.</p><h2 id=methodology>Methodology<a hidden class=anchor aria-hidden=true href=#methodology>#</a></h2><p>MLS is a pretty complex protocol with many moving parts.
It is therefore important to clearly define what is being measured and how.</p><p>First, all tests are done with the only mandatory cipher suite in MLS 1.0 MLS10_128_DHKEMX25519_AES128GCM_SHA256_Ed25519.
While other cipher suites obviously have different performance, the goal here is to investigate the general performance of MLS depending on the group size.
The exact cipher suite used is therefore irrelevant.</p><h3 id=measurements>Measurements<a hidden class=anchor aria-hidden=true href=#measurements>#</a></h3><p>The measurements here do not cover all possible messages in MLS.
Not all of them are fully supported by OpenMLS yet.
Pre-shared key, re-init, external-init, app-ack, and external proposals will be checked once they are implemented.
The measured messages nonetheless represent the core of the MLS protocol and should give a good idea of the general performance of the protocol.
We test performance of group creation, group join as well as the three basic messages update, add, and remove, and application messages.</p><p>All measurements except for the first two use one of the following set-ups:</p><ol><li><p><strong>Base</strong>: The group is created by a user.
All other participants are invited and each participant creates the group locally.
Then every participant sends an update message to the group and everyone else processes it.</p></li><li><p><strong>Bare:</strong> The group is created by a user.
All other participants are invited and each participant creates the group locally.
This creates an extremely sparse version of the underlying tree in MLS and is therefore interesting to look at.</p></li></ol><p>Measurements are run on different group sizes.
When running benchmarks with large groups such as 1000 participants a lot of memory is used in order to simulate all devices (up to 10 GB) such that larger groups are hard to simulate.
The chosen group sizes allow us to get a good idea how MLS performs depending on the group size.
We in particular test groups of the size 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000.</p><h4 id=operations>Operations<a hidden class=anchor aria-hidden=true href=#operations>#</a></h4><ul><li><strong>Group creation:</strong>
Creating a group involves creating the group, proposals and welcome messages for the other participants, and applying the commit.</li><li><strong>Join group:</strong>
Joining a group is equivalent to processing a welcome message to locally create the new group.</li><li><strong>Update messages:</strong>
Sending an update to a group involves creating a proposal, the corresponding commit and applying the commit.
When receiving an update message the commit is being processed.</li><li><strong>Adding a user:</strong>
When a new user is added to the group the add proposal and welcome message are created and the commit is locally applied by the adder.
When receiving a commit with an add proposal it is processed by the user.</li><li><strong>Removing a user:</strong>
When a user is removed from the group the remove proposal and commit are created and locally applied by the remover.
When receiving a commit with a remove proposal it is processed by the user.</li><li><strong>Application messages:</strong>
Sending an application message consists of creating the plaintext message and encrypting it for the group.
In order to receive an application message the user has to decrypt and parse the message.
We measure performance of a single message that is being sent and processed.
Note that the processing time of subsequent messages is not significantly different from the first one.</li></ul><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>You can find the raw data and some more graphs in the <a href=https://docs.google.com/spreadsheets/d/1nZv8lpT28JctDVo4ARBLZCKcIdvo-h8cIyN3_dIedFU>OpenMLS performance spreadsheet</a>.</p><p>All measurements were performed on a laptop with Arch Linux, an Intel Core i7-4900MQ @ 2.80GHz and 16 GB memory.</p><h3 id=group-setup>Group setup<a hidden class=anchor aria-hidden=true href=#group-setup>#</a></h3><p>As the following graph shows the time needed to create a group is linear in the number of participants added when creating the group.
The blue line shows the actual measurements while the magenta one is a trend line showing the linear relation.
This is what is to be expected because the performance is dominated by the creation of welcome messages, which have to be created for each member.</p><figure><a href=../images/group-creation.svg><img loading=lazy src=../images/group-creation.svg></a></figure><h3 id=join-group>Join group<a hidden class=anchor aria-hidden=true href=#join-group>#</a></h3><p>The performance of joining a group is linear in the group size because the information in the welcome message as well as the tree that is being processed when joining the group are linear in the number of group members.
Note that it is not logarithmic because the tree needs to be constructed.
This requires processing of each node in some way, which is linear in the group size.
The blue line again shows the actual measurements while the magenta one is a trend line for the linear relation.</p><figure><a href=../../images/join.svg><img loading=lazy src=../images/join.svg></a></figure><h3 id=update>Update<a hidden class=anchor aria-hidden=true href=#update>#</a></h3><p>Sending and processing updates are both sub-linear in the number of group members because the number of computations depend on the height of the tree in the <em>base</em> case.</p><figure><a href=../../images/update.svg><img loading=lazy src=../images/update.svg></a></figure><p>In the case of a very sparse tree, which we have in the <em>bare</em> case because every leaf only processed the welcome message, the performance of sending an update however is linear in the group size.
When creating a commit for an update proposal the sender has to include a path and refresh the private tree.
The following two flamegraphs show the difference between the base and the bare case.
While it doesn&rsquo;t show directly what&rsquo;s going on, it can be seen that in the base case (first flamegraph) the <code>new_with_keys</code> function requires a lot more time relative to the rest of the <code>replace_private_tree</code> function.
This is a strong indicator for where to look for the differences.</p><p><figure><a href=../images/send-update-base.png><img loading=lazy src=../images/send-update-base.png></a></figure><figure><a href=../images/send-update-bare.png><img loading=lazy src=../images/send-update-bare.png></a></figure></p><p>Looking at a tree with 300 leaves for example we have to encrypt 299 times (for every other leaf) in the case of a bare tree.
In a fully updated tree however only 9 encryptions are necessary, one for each level of the tree.
It is therefore expected that the performance of sending an update (with commit) in the bare case is linear in the group size.</p><h3 id=adding-a-user>Adding a user<a hidden class=anchor aria-hidden=true href=#adding-a-user>#</a></h3><p>Looking at the performance of adding a user and processing an add commit in the following graph we can again see the linear growth in relation to the number of group members.
This is almost independent of the state of the tree.
The operations appear to be slightly more expensive in a fully updated tree though.</p><figure><a href=../images/add.svg><img loading=lazy src=../images/add.svg></a></figure><h3 id=removing-a-user>Removing a user<a hidden class=anchor aria-hidden=true href=#removing-a-user>#</a></h3><p>Like updating, removing a user and processing a remove commit are linear in complexity in the base case as the following graph shows.
Removing in a very sparse tree is significantly more expensive than in a fully updated tree.
The reason is the same as for updating the tree.
The remove information has to be encrypted to all other remaining participants in the tree.</p><figure><a href=../images/remove.svg><img loading=lazy src=../images/remove.svg></a></figure><h3 id=application-messages>Application messages<a hidden class=anchor aria-hidden=true href=#application-messages>#</a></h3><p>Sending and receiving application messages is essentially independent of the group size, as expected.
Receiving the first message within an epoch has a small overhead compared to subsequent message as seen in the second graph.
This should be negligible in practice though.</p><p><figure><a href=../images/application-message-send.svg><img loading=lazy src=../images/application-message-send.svg></a></figure><figure><a href=../images/application-message-receive.svg><img loading=lazy src=../images/application-message-receive.svg></a></figure></p><h2 id=analysis>Analysis<a hidden class=anchor aria-hidden=true href=#analysis>#</a></h2><p>First, the plain performance numbers tell us that the goal of the MLS charter of a protocol that scales well for large groups has been mostly.
Depending on the state of the tree some operations might take longer than expected.
However, this can be mitigated by the application ensuring that the tree is updated and shrunk regularly.
Notably, the real world performance appears to be consistent with the theoretical expectations.</p><h2 id=technical-background>Technical background<a hidden class=anchor aria-hidden=true href=#technical-background>#</a></h2><p>The measurements are <em>not</em> done with any Rust benchmarking framework such as <a href=https://crates.io/crates/criterion>criterion</a>.
Due to the way criterion works there&rsquo;s significant <a href=https://github.com/bheisler/criterion.rs/issues/475>overhead in criterion</a>.
While the numbers in this post can be reliably reproduced a more thorough measurement framework</p><p>The flamegraphs are produced with <a href=https://crates.io/crates/pprof>pprof</a>, a simple to use CPU profiler for Rust.</p><p>All measurements were performed on <a href=https://github.com/openmls/openmls/tree/65030529e43716d482a6e57a432da5a388fd0a3c>this revision</a>.
To reproduce them check out the revision and run</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#66d9ef>for</span> i in <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>4</span> <span style=color:#ae81ff>5</span> <span style=color:#ae81ff>6</span> <span style=color:#ae81ff>7</span> <span style=color:#ae81ff>8</span> <span style=color:#ae81ff>9</span> <span style=color:#ae81ff>10</span> <span style=color:#ae81ff>20</span> <span style=color:#ae81ff>30</span> <span style=color:#ae81ff>40</span> <span style=color:#ae81ff>50</span> <span style=color:#ae81ff>100</span> <span style=color:#ae81ff>200</span> <span style=color:#ae81ff>300</span> <span style=color:#ae81ff>400</span> <span style=color:#ae81ff>500</span> 1000; <span style=color:#66d9ef>do</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    cargo bench --bench group -- $i; <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span><span style=color:#66d9ef>done</span>
</span></span></code></pre></div><h2 id=conclusion--future-work>Conclusion & Future work<a hidden class=anchor aria-hidden=true href=#conclusion--future-work>#</a></h2><p>Measuring performance of a protocol as complex as MLS is pretty difficult.
Without an application and elaborate test framework that can simulate many different scenarios it is only possible to get the basic numbers as shown here.
While they give a good indicator towards the performance of the MLS protocol they are insufficient to claim any performance of real applications that use MLS.</p><p>Nonetheless, the numbers show that the MLS protocol appears to allow for efficient, end-to-end-encrypted messaging in large groups.
Sending and receiving application messages is independent of the group size while group operations are sub-linear in the group size in most cases.</p><p>When OpenMLS is further developed and we have a messaging client using it another set of measurements should be performed with real world usage scenarios in mind in order to investigate whether the performance we have seen here translates to efficient group messaging in an application.
The MLS specification further leaves anything around authentication and authorization policies open to the application.
These might be complex procedures and impact the MLS performance as well.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.openmls.tech/tags/mls/>Mls</a></li><li><a href=https://blog.openmls.tech/tags/openmls/>Openmls</a></li><li><a href=https://blog.openmls.tech/tags/performance/>Performance</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://blog.openmls.tech/>OpenMLS</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>